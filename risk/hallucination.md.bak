# ðŸ¤– Risiko Hallucination LLM

Hallucination adalah fenomena ketika model bahasa menghasilkan jawaban yang **tidak akurat** atau **tidak berbasis data**.

## Dampak
- Menurunkan kepercayaan pengguna.
- Potensi kesalahan keputusan bisnis/kritis.

## Framework Rujukan
- NIST AI RMF (2023) â€“ *Validity & Reliability*.  
- STRANAS AI Indonesia (2020) â€“ *Aspek Kepercayaan Publik*.  

## Mitigasi
- **Prompt Engineering & Testing**  
- **Retrieval-Augmented Generation (RAG)**  
- **Human-in-the-Loop Validation**
